1) document_reader.py


   a) read_documents()  : 

    This method reads the documents in the folder in the folder in sequence.
    Also the entire text is converted to lower case. 

    It returns a list of documents. 

   b) remove_punctuations(documents) : 

    This method removes punctuations for each document within the documents list. Also the entire text is converted to lower case.
    Line spaces are also removed. 

    It returns a list of preprocessed documents 

   c) read_query(name): 

    This method reads the queries from a file. 

    It returns a list of queries. 

   d) read_relevance(name):

    This method reads the given relevance document provided and splits based on the line break. 

    It returns a list of relanvces as a query number and document number

   e) get_required_documents(abstract_folder_name,gold_folder_name) : 

    This method return the list of file names which have both gold and abstracts 

    It returns of list of file names

   f) read_abstracts(required_abstracts,abstract_folder_name) : 

    This method reads the files from the folder only which are required. 

    It returns the a list of the contents of the required files

   g) read_json_files(folder_name) : 

    Reads the json files from a given location. 

    It returns a lit of dictionaries.  
    
2) file_extractor.py


   a) extract_files()  :

    This method extracts the compressed folder to the working directory.
    It works for .zip and .tar.gz files.

    It returns nothing. 


3) get_top_contributors.py


    a) get(words_list, number, unique_words_length) :

    This method gets the top words contributing to [number] percent of the words in the [words_list]

    It returns a list of words.


4) most_common_words.py


    a) get_top(words_list, quantity)  :

    This method gets the top [quantity] words for the frequency dictionary of the [words_list]. 

    It returns a list of dictionary having the word and its frequency.


    b) get_top_list(words_list,quantity)  : 

    This method return the top [quantity] words for the frequency dictionary of the [words_list].

    It returns a list of words. 


5) stemmer.py


    a) get_stemmed_words(words_list)  :
    
    This method uses the Potter stemmer to stem words. 

    It returns a list of stemmed words. 

    b) remove_small_words(words_list) : 

    This method removes words of size lesser than 3 in each document and also removes numbers from the document if any. 

    It returns a list documents

    c) stem_documents(documents) :

    This method stems each document in the documents list. 

    It return a list of stemmed documents

    d) get_required_text(documents): 

    This method filter out the words with only the required POS tags. 

    It return a list of documents

    e) get_text(documents): 

    This method removes the POS tags from all documents and return only the actual words

    It returns a list of documents

    f) stem_phrases(documents):

    This method stems each word in the gold document

    It returns a list of document phrases




6) stopword_remover.py 


   a) remove(words_list)  :
   
    This method reads the stopwords from the stopwords.txt file and removes the stopwords from each document.

    It returns a list of documents.

   b) get_difference(words_list)  :
   
    This method return the words in words_list which are also stopwords. 

    It returns a list of words. 

  c) remove_from_document(documents) : 

   This method removes stopwords from a list of documents

   It returns a list of documents


7) tokenizer.py 


    a) get_all_tokens(documents) :

    This method tokenizes the [documents] using the nltk tokenizer. 

    It returns a list of words.

   b) get_unique_tokens(words_list) :
   
    This method gets all the uniqure tokens in the [word_list]

    It returns a list of words.

   c) tokenize_documents(documents) : 

    This method tokenizes each document within the documents list. 

    It return a list of tokenized documents. 


8) file_parser.py

 
   a) get_elements(documents) : 

    This method uses BeautifulSoup to fetch the title and text elements in a html file. 

    It return a list of documents. 

   b) get_links(documents) :

    This method gets the list of links from each document in the list. 

    It returns a list of links. 

   c) get_document_text(documents) :

    This method gets the text from each document in the list. 

    It returns a list of text. 

   d) get_out_links(documents) :

    This method gets the list of out_links from each document.  

    It returns a list of out_links. 

9) cosine_similarity_calculator.py

   a) cosine_similarity(query_weights, documents, document_weights, squared_document_weights) : 

    This method calculates the cosine similarity between the query and documents. 

    It returns a cosine similarity dictionary

   b) sort_cosine_similarity(cosine_similarities) : 

    This method sorts the given cosine similarity dictionary in sorting order based on the values. 

    It returns a sorted cosine similarity dictionary

   c) calculate_precision_recall(sorted_cos_sim, top_n , query_number, relevances) :

    This method calculates the precision and recall by validating the retrieved documents against the relevant ones. 

    It returns the precision and recall. 


10. tfidf_calculatory.py


   a) calculate_document_tfidf(documents,dictionary) : 

    This method calculates the weights (tf-idf) for the documents.

    It returns the tf-idf dictionary and also a list of sum of squared weights of each document. 

   b) calculate_query_tfidf(queries,documents,dictionary) : 

    This method calculates the weights (tf-idf) for the queries. 

    It returns the tf-idf dictionary for queries. 


11. word_document_frequency.py


   a) build_dictionary(documents): 

    This method builds the inverted document dictionary for the documents. 

    It returns the inverted dictionary


12. graphy.py


   a) create_graph(node_documents) : 

    This method creates a networkx graph for each document. 

    It returns a list of networkx graphs. 

   b) add_edges(document_graphs,original_documents,window_size) :

    This method adds edges to the document graphs based on the window size proximity. 

    It returns a list of networkx graph with undirected weighted edges

   c) created_directed_graph(node_documents)  : 

    This methods creatted a directed networkx graph. 

    It returns a a list of networkx graphs. 

   d) add_link_edges(graph,node_links,out_links) :

    This method adds directed edges to the graph. 

    It returns a list of networkx graph with directed edges  

   
13. scorer.py


   a) calculate_word_score(graphs) : 

    This method calculates the page rank score for each node in the graph. 

    It return a dictionary of nodes in each document and their scores. 

   b) calculate_ngram_score(documents, scores) : 

    This method creates all the ngram phrases for the documents and calculates their scores based on individual word score. 

    It return a dictionary of sorted phrase scores 

   c) calculate_mrr_score(documents_sorted_phrase_score, gold_documents) : 

    This method calculates the mrr score for the documents and return its average

    It returns a list of mrr averages for each value of k

   d) calculate_page_score(graph) : 

    This method calculates the page rank score for the documents. 

    It returns a dictionary of scores. 

   e) calculate_combined_scores(cos_sim_map,page_scores):

    This method combines the page rank and cosine similarity scores and returns the sorted list. 

    It returns a dictionary of scores.   


14. uic_scraper : 

    a) files : 
    
    Contains all the scraped documents in json format. 

    b) uic_scraper : 

    A scrapy project to crawl and index the files

    c) runner.py : 

    A launcher program to start uic_scraper

    d) scrapy.cfg : 

    A config file for the scrapper. 

    e) uic_docs.csv : 

    Contains links of all the documents scraped. 

